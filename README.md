# Multimodal JEPA with Attention-Based Label-Patch Interaction

This implementation provides a multimodal Joint-Embedding Predictive Architecture (JEPA) where class label tokens interact with image patch tokens through self-attention in a Vision Transformer.

## üìù Experiment Note: Can multimodal JEPA enable unsupervised retrieval?

I ran a small MNIST experiment to check whether a multimodal JEPA (image patches + label tokens in the same attention stack) can *support* unsupervised label retrieval. The goal here isn‚Äôt to claim a proof ‚Äî just to see if this interaction pattern yields meaningful retrieval signals beyond chance.

**Setup (brief):**
- Dataset: MNIST (60k train / 10k test)
- Model: ViT-style encoder with a label token and self-attention interaction
- Training: 50 epochs, multimodal mode on
- Evaluation: linear probe + two retrieval methods (M1 and M2)

**Placeholder for figure:**
<!-- TODO: add training curves / retrieval chart image here -->

**What I observed:**
- Linear probe quickly reached ~0.95 accuracy, suggesting the representations stayed class-informative.
- Retrieval accuracy was consistently above random (10%), with noticeable fluctuations across epochs.
- Peak values in this run were around:
  - **M1** top-1 ‚âà 0.79 (top-3 ‚âà 0.96)
  - **M2** top-1 ‚âà 0.64 (top-3 ‚âà 0.88)

**Interpretation (lightweight):**
This run suggests that letting label tokens and patch tokens interact through attention can produce embeddings that carry enough structure for unsupervised label retrieval on MNIST. It doesn‚Äôt prove generality, but it‚Äôs a useful signal that the multimodal JEPA objective can align modalities in a retrieval-friendly way, at least in this small-scale setting.

**Caveats:**
- Single dataset and single run.
- Retrieval scores vary with epoch; picking a checkpoint matters.
- MNIST is a simple benchmark; stronger evidence would require more datasets and ablations.

If you want to replicate, the full config and scripts are below, and I‚Äôll drop the plot once I add the figure.

## Architecture Overview

### Key Innovation: Attention-Based Multimodal Fusion

Unlike simple concatenation or addition, this architecture allows **bidirectional interaction** between patches and labels:

```
Input: Image (32x32) + Label (0-9)
   ‚Üì
1. PatchEmbed: Image ‚Üí [patch_1, patch_2, ..., patch_64] 
   (64 patches for 4x4 patch size on 32x32 image)

2. LabelEmbed: Label ‚Üí [label_token]

3. Concatenate: [patch_1, ..., patch_64, label_token] (65 tokens)

4. Add positional embeddings

5. Transformer Blocks (√ó6):
   ‚Ä¢ Self-attention: Each patch can attend to the label
   ‚Ä¢ Self-attention: Label can attend to all patches
   ‚Ä¢ Patches condition on semantic label information
   ‚Ä¢ Label aggregates visual information from patches

6. Pool and project to final representation
```

### Components

**`MultimodalViTEncoder`**: Custom ViT with integrated label token support
- `forward_features()`: Adds label token before transformer blocks
- Transformer blocks use `timm.models.vision_transformer.Block` (same quality as standard ViT)
- Self-attention allows full interaction between all tokens

**`MultimodalViTLeJEPA`**: Encoder wrapper with three modes:
- `encode_image(x)`: Patches only (no label token)
- `encode_label(labels)`: Label embedding only
- `encode_multimodal(x, labels)`: Full attention-based interaction

**`MultimodalLeJEPAModel`**: Complete model with LeJEPA loss
- Invariance loss: Minimize variance across augmented views
- SIGReg: Regularization for Gaussian structure
- Same loss as your privacy experiments

## Files

- `multimodal_models.py`: Model architecture (17KB)
- `retrieval_eval.py`: Evaluation code (9KB)
- `train_multimodal_retrieval.py`: Training script (17KB)
- `plot_results.py`: Plotting utilities (6KB)

## Usage

### Quick Start

```bash
# Train with multimodal attention (default)
python train_multimodal_retrieval.py \\
    --output_dir ./experiments/multimodal \\
    --epochs 50 \\
    --batch_size 128 \\
    --lr 1e-3 \\
    --depth 6 \\
    --num_heads 3 \\
    --emb_dim 192 \\
    --patch_size 4

# Baseline (no labels) for comparison
python train_multimodal_retrieval.py \\
    --output_dir ./experiments/baseline \\
    --no_multimodal \\
    --epochs 50

# Plot results
python plot_results.py \\
    --metrics_csv ./experiments/multimodal/metrics.csv \\
    --output_dir ./experiments/multimodal/plots
```

### Hyperparameters

**Architecture:**
- `--emb_dim`: Transformer embedding dimension (default: 192)
- `--depth`: Number of transformer blocks (default: 6)
- `--num_heads`: Number of attention heads (default: 3)
- `--patch_size`: Patch size for tokenization (default: 4)
- `--proj_dim`: Projection head output dimension (default: 64)

**Training:**
- `--epochs`: Number of training epochs (default: 50)
- `--batch_size`: Batch size (default: 128)
- `--lr`: Learning rate (default: 1e-3)
- `--no_multimodal`: Disable label tokens (baseline)

**Evaluation:**
- `--eval_every`: Evaluate every N epochs (default: 5)

## Retrieval Methods

### Method 1: Image-only vs Image+Label Matching

Tests whether the model learns to associate labels with visual content through attention:

1. Encode image alone: `E_img = Encoder(patches)`
2. For each label k: `E_img+k = Encoder(patches + label_k)` ‚Üê **Attention interaction!**
3. Retrieved label = `argmax_k similarity(E_img, E_img+k)`

**Interpretation**: Which label, when attended to by the patches, produces an embedding closest to the image-only embedding?

### Method 2: Patch-only vs Label-only Matching

Direct matching in the shared embedding space:

1. `E_patches = Encoder(patches)`
2. `E_labels = [Encoder(label_k) for k in 0..9]`
3. Retrieved label = `argmax_k similarity(E_patches, E_labels[k])`

**Interpretation**: Which standalone label embedding is closest to the patch representation?

## Expected Results

### Baseline (no multimodal training):
- Retrieval accuracy: ~10% (random guessing)
- Linear probe: >95% (representations preserve class information)

### With multimodal training:
- **Method 1**: 60-85% top-1 accuracy
  - Tests compositional understanding
  - Requires attention-based reasoning
- **Method 2**: 70-90% top-1 accuracy
  - Simpler direct matching
  - Often higher than Method 1
- **Linear probe**: >95% (multimodal training doesn't hurt discriminability)

### Training dynamics:
- Epochs 0-10: Retrieval improves rapidly from random
- Epochs 10-30: Gradual improvement and stabilization
- Epochs 30-50: Fine-tuning, minor improvements

## Example Output

```
Epoch 5/50 | Loss: 0.3421 | Inv: 0.2134 | SIG: 0.5987 | 45.2s
  Evaluating...
  Linear: 0.9234
  M1: 0.6523 (top3: 0.8456)
  M2: 0.7891 (top3: 0.9123)

Epoch 10/50 | Loss: 0.2876 | Inv: 0.1745 | SIG: 0.5234 | 44.8s
  Evaluating...
  Linear: 0.9567
  M1: 0.7234 (top3: 0.8923)
  M2: 0.8345 (top3: 0.9456)
```

# Add retrieval evaluation
from retrieval_eval import LabelRetrievalEvaluator

evaluator = LabelRetrievalEvaluator(lejepa_model, device=device)
results = evaluator.evaluate_retrieval(test_loader, max_batches=10)

logger.info(f"Retrieval M1: {results['method1']['accuracy']:.2%}")
logger.info(f"Retrieval M2: {results['method2']['accuracy']:.2%}")
```

## Comparison: Attention vs Concat/Add

**Previous approach** (concat/add):
```python
# Concat: [patches, label] (no interaction before projection)
# Add: patches + label (element-wise, limited interaction)
proj = MLP(concat([patch_emb, label_emb]))
```

**This approach** (attention):
```python
# Label token added before transformer
tokens = [patch_1, ..., patch_N, label]

# Self-attention allows bidirectional interaction
for block in transformer_blocks:
    tokens = SelfAttention(tokens)  # Patches ‚Üî Label
    
# Now patches are conditioned on label, label on patches
proj = MLP(pool(tokens))
```

**Benefits:**
1. Patches can query label for semantic guidance
2. Label can query patches for visual evidence  
3. More expressive multimodal fusion
4. Closer to how language-vision models (CLIP, BERT) work

## Architecture Diagram

```
                        Input
                          |
        +----------------+----------------+
        |                                 |
    Image (32x32)                    Label (0-9)
        |                                 |
   PatchEmbed                       LabelEmbed
        |                                 |
[patch_1, ..., patch_64]            [label_token]
        |                                 |
        +----------------+----------------+
                         |
                   Concatenate
                         |
            [patch_1, ..., patch_64, label]
                         |
              + Positional Embeddings
                         |
              Transformer Block 1
              (Self-Attention + FFN)
                         |
              Transformer Block 2
                         |
                       ...
                         |
              Transformer Block 6
                         |
                  LayerNorm
                         |
                   Mean Pool
                         |
                Projection Head
                         |
              Final Representation
                     (proj_dim)
```

## Experiments to Run

### Experiment A: Validation
```bash
# Baseline: No labels ‚Üí ~10% retrieval
python train_multimodal_retrieval.py --no_multimodal --output_dir exp_baseline --epochs 30
```

### Experiment B: Multimodal training
```bash
# With attention ‚Üí 70-90% retrieval
python train_multimodal_retrieval.py --output_dir exp_multimodal --epochs 50
```

### Experiment C: Architecture ablations
```bash
# Shallow (3 blocks)
python train_multimodal_retrieval.py --depth 3 --output_dir exp_shallow

# Deep (12 blocks)
python train_multimodal_retrieval.py --depth 12 --output_dir exp_deep

# Small patches (2x2)
python train_multimodal_retrieval.py --patch_size 2 --output_dir exp_small_patch

# Large patches (8x8)
python train_multimodal_retrieval.py --patch_size 8 --output_dir exp_large_patch
```

### Experiment D: Analysis
```bash
# Generate confusion matrices
python -c "
from retrieval_eval import LabelRetrievalEvaluator
from multimodal_models import MultimodalLeJEPAModel
import torch

model = MultimodalLeJEPAModel(...)
model.load_state_dict(torch.load('checkpoint.pt')['model_state_dict'])

evaluator = LabelRetrievalEvaluator(model)
conf = evaluator.compute_confusion_matrix(test_images, test_labels, method=1)

import matplotlib.pyplot as plt
import seaborn as sns
sns.heatmap(conf, annot=True, fmt='d')
plt.savefig('confusion_method1.png')
"
```

## Citations

This implementation combines ideas from:
- LeJEPA (LeCun et al.)

## Requirements

```
torch>=2.0
torchvision
timm
numpy
matplotlib
seaborn
pandas
```

Install:
```bash
pip install torch torchvision timm numpy matplotlib seaborn pandas
```